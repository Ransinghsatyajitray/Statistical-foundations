1. #The Probability of the Type 2 error and sample size calculation:
Type1 Error: wrongly rejecting the Ho(rejecting the true claim)
Type2 Error: wrongly accepting the Ho(failing to reject the false claim)

Power: the probability of falsely rejecting the Ho , when it is false is known as the power of the test. The larger the better.

2. Supervised statistical learning involves building a statistical model for
predicting, or estimating, an output based on one or more inputs.

With unsupervised statistical learning, there are
inputs but no supervising output; nevertheless we can learn relationships
and structure from such data.

3. Generalised Linear Model = linear & logistic regression

4. In mid 1980’s Breiman, Friedman, Olshen and Stone introduced classification and
regression trees, and were among the first to demonstrate the power of a
detailed practical implementation of a method, including cross-validation
for model selection.

5. Hastie and Tibshirani coined the term generalized additive models in 1986 for a class of non-linear extensions to generalized linear
models, and also provided a practical software implementation.

6. The inputs go by different names, such as 
predictors, 
independent variables, 
features, or
variables.

7. The output variable
response or dependent variable

Y = f(X) + e

8. Why Estimate f?
There are two main reasons that we may wish to estimate for f: 
prediction
inference.

9. Prediction
In many situations, a set of inputs X are readily available, but the output
Y cannot be easily obtained. In this setting, since the error term averages
to zero, we can predict Y using
ˆY = ˆ f(X), (2.2)
where ˆ f represents our estimate for f, and ˆ Y represents the resulting prediction
for Y . 
Very Important:
____________________________________
In this setting, ˆ f is often treated as a black box, in the sense
that one is not typically concerned with the exact form of ˆ f, provided that
it yields accurate predictions for Y .

10. The accuracy of ˆY as a prediction for Y depends on two quantities,
which we will call the reducible error and the irreducible error.

11. reducible error
This error is reducible because we can potentially improve the
accuracy of ˆ f by using the most appropriate statistical learning technique to
estimate f. 

12. irreducible error.
However, even if it were possible to form a perfect estimate for
f, so that our estimated response took the form ˆ Y = f(X), our prediction
would still have some error in it! This is because Y is also a function of
ǫ, which, by definition, cannot be predicted using X. Therefore, variability
associated with ǫ also affects the accuracy of our predictions. This is known
as the irreducible error, because no matter how well we estimate f, we
cannot reduce the error introduced by ǫ.


13. Why is the irreducible error larger than zero? The quantity ǫ may contain
unmeasured variables that are useful in predicting Y : since we don’t
measure them, f cannot use them for its prediction. The quantity ǫ may
also contain unmeasurable variation. For example, the risk of an adverse
reaction might vary for a given patient on a given day, depending on manufacturing
variation in the drug itself or the patient’s general feeling of
well-being on that day.

14. It is important to keep in mind that the
irreducible error will always provide an upper bound on the accuracy of
our prediction for Y . This bound is almost always unknown in practice.

15.Inference:
We are often interested in understanding the way that Y is affected as
X1, . . . ,Xp change. In this situation we wish to estimate f, but our goal is
not necessarily to make predictions for Y . We instead want to understand
the relationship between X and Y , or more specifically, to understand how
Y changes as a function of X1, . . . ,Xp. Now ˆ f cannot be treated as a black
box, because we need to know its exact form.

16. Broadly speaking, most statistical learning methods for this task can be characterized
as either parametric or non-parametric.

17. Parametric methods
Parametric methods involve a two-step model-based approach.
> 1. First, we make an assumption about the functional form, or shape,
of f.
> 2. After a model has been selected, we need a procedure that uses the
training data to fit or train the model.

The model-based approach just described is referred to as parametric;
it reduces the problem of estimating f down to one of estimating a set of
parameters.
The potential disadvantage of a parametric
approach is that the model we choose will usually not match the true
unknown form of f.
These more complex models can lead to a
phenomenon known as overfitting the data, which essentially means they
overfitting
follow the errors, or noise, too closely

18. Non-parametric methods
Non-parametric methods do not make explicit assumptions about the functional
form of f. Instead they seek an estimate of f that gets as close to the
data points as possible without being too rough or wiggly. Such approaches
can have a major advantage over parametric approaches: by avoiding the
assumption of a particular functional form for f, they have the potential
to accurately fit a wider range of possible shapes for f.

19. The Trade-off Between Prediction Accuracy and Model
Interpretability
Of the many methods that we examine some are less flexible,
or more restrictive, in the sense that they can produce just a relatively
small range of shapes to estimate f.

For example, linear regression is a
relatively inflexible approach, because it can only generate linear functions
Other methods, such as the thin plate splines are considerably more flexible because they can generate a much wider
range of possible shapes to estimate f.
One might reasonably ask the following question: why would we ever
choose to use a more restrictive method instead of a very flexible approach?
If we are mainly interested in inference, then restrictive models are much more interpretable.

For instance, when inference is the goal, the linear
model may be a good choice since it will be quite easy to understand
the relationship between Y and X1,X2, . . . ,Xp. In contrast, very flexible
approaches, such as the splines , can lead to such complicated estimates of f that it is difficult to understand
how any individual predictor is associated with the response.

In general, as the flexibility of a method increases, its interpretability decreases.
Least squares linear regression, is relatively inflexible but is quite interpretable.

The lasso,  relies upon the lasso linear model but uses an alternative fitting procedure for estimating
the coefficients β0, β1, . . . , βp. The new procedure is more restrictive in estimating
the coefficients, and sets a number of them to exactly zero. Hence
in this sense the lasso is a less flexible approach than linear regression.It is also more interpretable than linear regression, because in the final
model the response variable will only be related to a small subset of the
predictors — namely, those with nonzero coefficient estimates.
They are also somewhat less interpretable than linear regression, because the relationship between
each predictor and the response is now modeled using a curve.

Finally, fully non-linear methods such as bagging, boosting, and support vector machines
with non-linear kernels,are highly flexible approaches that are harder to interpret.

20. We have established that when inference is the goal, there are clear advantages
to using simple and relatively inflexible statistical learning methods. 

21. In some settings, however, we are only interested in prediction, and
the interpretability of the predictive model is simply not of interest.

