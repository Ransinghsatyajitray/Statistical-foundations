1. #The Probability of the Type 2 error and sample size calculation:
Type1 Error: wrongly rejecting the Ho(rejecting the true claim)
Type2 Error: wrongly accepting the Ho(failing to reject the false claim)

Power: the probability of falsely rejecting the Ho , when it is false is known as the power of the test. The larger the better.

2. Supervised statistical learning involves building a statistical model for
predicting, or estimating, an output based on one or more inputs.

With unsupervised statistical learning, there are
inputs but no supervising output; nevertheless we can learn relationships
and structure from such data.

3. Generalised Linear Model = linear & logistic regression

4. In mid 1980’s Breiman, Friedman, Olshen and Stone introduced classification and
regression trees, and were among the first to demonstrate the power of a
detailed practical implementation of a method, including cross-validation
for model selection.

5. Hastie and Tibshirani coined the term generalized additive models in 1986 for a class of non-linear extensions to generalized linear
models, and also provided a practical software implementation.

6. The inputs go by different names, such as 
predictors, 
independent variables, 
features, or
variables.

7. The output variable
response or dependent variable

Y = f(X) + e

8. Why Estimate f?
There are two main reasons that we may wish to estimate for f: 
prediction
inference.

9. Prediction
In many situations, a set of inputs X are readily available, but the output
Y cannot be easily obtained. In this setting, since the error term averages
to zero, we can predict Y using
ˆY = ˆ f(X), (2.2)
where ˆ f represents our estimate for f, and ˆ Y represents the resulting prediction
for Y . 
Very Important:
____________________________________
In this setting, ˆ f is often treated as a black box, in the sense
that one is not typically concerned with the exact form of ˆ f, provided that
it yields accurate predictions for Y .

10. The accuracy of ˆY as a prediction for Y depends on two quantities,
which we will call the reducible error and the irreducible error.

11. reducible error
This error is reducible because we can potentially improve the
accuracy of ˆ f by using the most appropriate statistical learning technique to
estimate f. 

12. irreducible error.
However, even if it were possible to form a perfect estimate for
f, so that our estimated response took the form ˆ Y = f(X), our prediction
would still have some error in it! This is because Y is also a function of
ǫ, which, by definition, cannot be predicted using X. Therefore, variability
associated with ǫ also affects the accuracy of our predictions. This is known
as the irreducible error, because no matter how well we estimate f, we
cannot reduce the error introduced by ǫ.


13. Why is the irreducible error larger than zero? The quantity ǫ may contain
unmeasured variables that are useful in predicting Y : since we don’t
measure them, f cannot use them for its prediction. The quantity ǫ may
also contain unmeasurable variation. For example, the risk of an adverse
reaction might vary for a given patient on a given day, depending on manufacturing
variation in the drug itself or the patient’s general feeling of
well-being on that day.

14. It is important to keep in mind that the
irreducible error will always provide an upper bound on the accuracy of
our prediction for Y . This bound is almost always unknown in practice.

15.Inference:
We are often interested in understanding the way that Y is affected as
X1, . . . ,Xp change. In this situation we wish to estimate f, but our goal is
not necessarily to make predictions for Y . We instead want to understand
the relationship between X and Y , or more specifically, to understand how
Y changes as a function of X1, . . . ,Xp. Now ˆ f cannot be treated as a black
box, because we need to know its exact form.

16. Broadly speaking, most statistical learning methods for this task can be characterized
as either parametric or non-parametric.

17. Parametric methods
Parametric methods involve a two-step model-based approach.
> 1. First, we make an assumption about the functional form, or shape,
of f.
> 2. After a model has been selected, we need a procedure that uses the
training data to fit or train the model.

The model-based approach just described is referred to as parametric;
it reduces the problem of estimating f down to one of estimating a set of
parameters.
The potential disadvantage of a parametric
approach is that the model we choose will usually not match the true
unknown form of f.
These more complex models can lead to a
phenomenon known as overfitting the data, which essentially means they
overfitting
follow the errors, or noise, too closely

18. Non-parametric methods
Non-parametric methods do not make explicit assumptions about the functional
form of f. Instead they seek an estimate of f that gets as close to the
data points as possible without being too rough or wiggly. Such approaches
can have a major advantage over parametric approaches: by avoiding the
assumption of a particular functional form for f, they have the potential
to accurately fit a wider range of possible shapes for f.

19. The Trade-off Between Prediction Accuracy and Model
Interpretability
Of the many methods that we examine some are less flexible,
or more restrictive, in the sense that they can produce just a relatively
small range of shapes to estimate f.

For example, linear regression is a
relatively inflexible approach, because it can only generate linear functions
Other methods, such as the thin plate splines are considerably more flexible because they can generate a much wider
range of possible shapes to estimate f.
One might reasonably ask the following question: why would we ever
choose to use a more restrictive method instead of a very flexible approach?
If we are mainly interested in inference, then restrictive models are much more interpretable.

For instance, when inference is the goal, the linear
model may be a good choice since it will be quite easy to understand
the relationship between Y and X1,X2, . . . ,Xp. In contrast, very flexible
approaches, such as the splines , can lead to such complicated estimates of f that it is difficult to understand
how any individual predictor is associated with the response.

In general, as the flexibility of a method increases, its interpretability decreases.
Least squares linear regression, is relatively inflexible but is quite interpretable.

The lasso,  relies upon the lasso linear model but uses an alternative fitting procedure for estimating
the coefficients β0, β1, . . . , βp. The new procedure is more restrictive in estimating
the coefficients, and sets a number of them to exactly zero. Hence
in this sense the lasso is a less flexible approach than linear regression.It is also more interpretable than linear regression, because in the final
model the response variable will only be related to a small subset of the
predictors — namely, those with nonzero coefficient estimates.
They are also somewhat less interpretable than linear regression, because the relationship between
each predictor and the response is now modeled using a curve.

Finally, fully non-linear methods such as bagging, boosting, and support vector machines
with non-linear kernels,are highly flexible approaches that are harder to interpret.

20. We have established that when inference is the goal, there are clear advantages
to using simple and relatively inflexible statistical learning methods. 

21. In some settings, however, we are only interested in prediction, and
the interpretability of the predictive model is simply not of interest.

For instance, if we seek to develop an algorithm to predict the price of a stock,
our sole requirement for the algorithm is that it predict accurately — interpretability
is not a concern. In this setting, we might expect that it will
be best to use the most flexible model available.

22. unsupervised learning:
We can seek to understand the relationships between the variables or
between the observations. One statistical learning tool that we may use in
this setting is cluster analysis, or clustering. The goal of cluster analysis is
cluster analysis to ascertain, on the basis of x1, . . . , xn, whether the observations fall into
relatively distinct groups.

23. Regression versus Classification Problems:
Variables can be characterized as either quantitative or qualitative.
>We tend to refer to problems with a quantitative response, as regression problems,
>those involving a qualitative response are often referred to as classification problems.

However, the distinction is notclassification always that crisp.
#Least squares linear regression  > quantitative response
#logistic regression > qualitative (two-class, or binary) response. classification method. But since it estimates class probabilities, it
can be thought of as a regression method as well.

>Some statistical methods,such as K-nearest neighbors and boosting can be used in the case of either quantitative or qualitative responses.


24. Why is it necessary to introduce so many different
statistical learning approaches, rather than just a single best method? There
is no free lunch in statistics: no one method dominates all others over all
possible data sets.


important concepts that arise in selecting a statistical learning procedure for a specific data set.
> Measuring the Quality of Fit:
In the regression setting, the most commonly-used measure is the mean squared error (MSE)


25. Bias Variance trade off:
Bias on the training data and the variance on the test data.
When the data fits the training data very well but not the testing data it is called overfit.
We want a model which has loe bias as well as low variance.

eg. consider a case where a linear regression fits a straight line to the training set but the straight line doesnt have the flexibility to accurately
replicate the arc in the true relationship.
=> st line will never capture the true relation between the ht and the wt np matter how well we fit to the training data set.

The inability of the m/c learning method to capture the true relationship is called the bias.

Another m/c learning method might be a squiggly line to the training set. the squiggly line is flexible and hugs the training set along 
the arc of the true relation. The squiggly line has very low bias.

But when we compare the models on the test data we see that on the test set the st line better fit the data. In the m/c learning the difference 
in the fits b/w the data sets is called variance.

Good model has low bias and low variance. This is achieved by finding the sweet spot between simple model and complex model.


It is clear that as the level of flexibility increases, the curves fit the observed
data more closely. A more restricted and hence smoother curve has fewer degrees
of freedom than a wiggly curve . linear regression
is at the most restrictive end, with two degrees of freedom.

26. Note that regardless of whether or not overfitting has
occurred, we almost always expect the training MSE to be smaller than
the test MSE because most statistical learning methods either directly or
indirectly seek to minimize the training MSE. Overfitting refers specifically
to the case in which a less flexible model would have yielded a smaller test
MSE.

27. What do we mean by the variance and bias of a statistical learning
method?
Variance refers to the amount by which ˆ f would change if we
estimated it using a different training data set. Since the training data
are used to fit the statistical learning method, different training data sets
will result in a different ˆ f. But ideally the estimate for f should not vary
too much between training sets. 

Generally, more flexible methods result in less bias.

28. As a general rule, as we use more flexible methods, the variance will
increase and the bias will decrease.

29. The relationship between bias, variance, and test set MSE is referred to as the bias-variance
trade-off.

30. The Classification Setting:
The most common approach for quantifying the accuracy of our estimate ˆ f is
the training error rate,
A good classifier is one for which the test error is smallest.

31. This is called the Bayes decision boundary. The Bayes classifier’s prediction is determined by the Bayes decision boundary. The Bayes classifier produces the lowest possible test error rate, called
the Bayes error rate. 
